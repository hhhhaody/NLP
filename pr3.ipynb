{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pr3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxe4Dr-WNdc3",
        "outputId": "6410de91-0ba5-4c98-8600-cb50b10289e4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "# !pip install bcolz\n",
        "import bcolz\n",
        "import pickle\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnN_MgAQ8Gd"
      },
      "source": [
        "traindata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sentiment-train.csv')\n",
        "testdata = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sentiment-test.csv')\n",
        "train_all = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/training.1600000.processed.noemoticon.csv',header=None,encoding='latin-1')\n",
        "train_all.loc[train_all[0] == 4, 0]=1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCWniuLwUg4O"
      },
      "source": [
        "def preprocess(data):\n",
        "  for i in range(len(data)):\n",
        "      data[i] = re.sub(r'https?:\\/\\/\\S*','url',data[i])\n",
        "      data[i] = re.sub(r'www?:\\/\\/\\S*','url',data[i])\n",
        "      data[i] = re.sub(r'@\\S*','@',data[i])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPxAKywsQt19"
      },
      "source": [
        "train_sentences = list(traindata.text)\n",
        "test_sentences = list(testdata.text)\n",
        "preprocess(train_sentences)\n",
        "preprocess(test_sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veWvqjt6RHgi"
      },
      "source": [
        "dic = Counter()\n",
        "for i, sentence in enumerate(train_sentences):\n",
        "  train_sentences[i] = []\n",
        "  for word in nltk.word_tokenize(sentence):\n",
        "    dic.update([word.lower()])\n",
        "    train_sentences[i].append(word.lower())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtlH2zeJRpt5"
      },
      "source": [
        "# Removing the words that only appear once\n",
        "vocab = {k:v for k,v in dic.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "vocab = ['_PAD','_UNK'] + vocab\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(vocab)}\n",
        "idx2word = {i:o for i,o in enumerate(vocab)}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVqmTeYksH33"
      },
      "source": [
        "for i, sentence in enumerate(train_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]\n",
        "\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    # For test sentences, we have to tokenize the sentences as well\n",
        "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else word2idx['_UNK'] for word in nltk.word_tokenize(sentence)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5igDy-yZs5xo"
      },
      "source": [
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
        "\n",
        "def pad_input(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTBiazeLuL-o"
      },
      "source": [
        "seq_len = 200 #The length that the sentences will be padded/shortened to\n",
        "\n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "test_sentences = pad_input(test_sentences, seq_len)\n",
        "\n",
        "# Converting our labels into numpy arrays\n",
        "train_labels = np.array(list(traindata.sentiment))\n",
        "test_labels = np.array(list(testdata.sentiment))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvBOywtNush3"
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 400\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAssD4vbvPKe",
        "outputId": "873b4a18-7592-4e2d-aefa-575251ec9253"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hub_JeWjvSq2"
      },
      "source": [
        "# dataiter = iter(train_loader)\n",
        "# sample_x, sample_y = dataiter.next()\n",
        "\n",
        "# print(sample_x.shape, sample_y.shape)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxbGEDakvVK3"
      },
      "source": [
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5, bidirectional = False):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.num_directions = num_directions\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers*self.num_directions, batch_size, self.hidden_dim).zero_().to(device),\n",
        "            weight.new(self.n_layers*self.num_directions, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx0mQpekIrBF"
      },
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, pretrain = None, drop_prob=0.5, bidirectional =False):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        num_directions = 2 if bidirectional else 1\n",
        "        self.num_directions = num_directions\n",
        "\n",
        "        if pretrain is None:\n",
        "          self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "          self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n",
        "        else:\n",
        "          (num,d) = pretrain.shape\n",
        "          self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(pretrain).float(),freeze=False)\n",
        "          self.gru = nn.GRU(d, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional = bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        out = self.sigmoid(out)\n",
        "        \n",
        "        out = out.view(batch_size, -1)\n",
        "        out = out[:,-1]\n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers*self.num_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDn0vgULv0Eu"
      },
      "source": [
        "vocab_size = len(word2idx)\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "lr=0.005\n",
        "epochs = 2\n",
        "clip = 5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxG-_YhmwGMi"
      },
      "source": [
        "def train(model, trainloader, epochs, batch_size, lr, clip, lstm = True,):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.BCELoss()\n",
        "  model.train()\n",
        "  for i in range(epochs):\n",
        "      h = model.init_hidden(batch_size)\n",
        "      \n",
        "      for inputs, labels in trainloader:\n",
        "        if lstm:\n",
        "          h = tuple([e.data for e in h])\n",
        "        else:\n",
        "          h = h.data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()      "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTOi7zmVAuDW"
      },
      "source": [
        "def eval(model,testloader,dim = 359,lstm = True):\n",
        "  criterion = nn.BCELoss()\n",
        "  test_losses = []\n",
        "  num_correct = 0\n",
        "  h = model.init_hidden(dim)\n",
        "\n",
        "  model.eval()\n",
        "  for inputs, labels in testloader:\n",
        "    if lstm:\n",
        "      h = tuple([each.data for each in h])\n",
        "    else:\n",
        "      h = h.data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "          \n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "  test_acc = num_correct/len(testloader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n",
        "  return test_acc"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peQpwUX15EVL"
      },
      "source": [
        "3.1 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru6jhnLXTnAq",
        "outputId": "81c44b62-3437-48f1-a005-a17cde8b5aad"
      },
      "source": [
        "model = LSTMNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMNet(\n",
            "  (embedding): Embedding(17150, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVC1KXKmMnv5"
      },
      "source": [
        "train(model,train_loader ,epochs,batch_size,lr,clip)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HDBhFqyMxki",
        "outputId": "927b1fad-36b7-42cd-dfc8-e1ed85792f0d"
      },
      "source": [
        "acc=eval(model,test_loader)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.488\n",
            "Test accuracy: 77.716%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgOcuQRA5Mp3"
      },
      "source": [
        "3.2 GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3EYpYAaB_ub",
        "outputId": "27db8077-7be2-4b35-a049-4f560eeed19d"
      },
      "source": [
        "model2 = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model2.to(device)\n",
        "print(model2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRUNet(\n",
            "  (embedding): Embedding(17150, 400)\n",
            "  (gru): GRU(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjrlRj7-JeXO"
      },
      "source": [
        "train(model2,train_loader,epochs,batch_size,lr,clip,lstm = False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiawlx41NtU7",
        "outputId": "2909a64e-e3bc-4894-ed4f-4cb226eda6e9"
      },
      "source": [
        "acc = eval(model2,test_loader,lstm = False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.493\n",
            "Test accuracy: 78.273%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMcy6Z7O5Qes"
      },
      "source": [
        "3.3 Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKaCsJ7QSnu1",
        "outputId": "0aa760bb-cc96-4c6c-a6b8-1b04b73d048a"
      },
      "source": [
        "model3 = LSTMNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional= True)\n",
        "model3.to(device)\n",
        "print(model3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTMNet(\n",
            "  (embedding): Embedding(17150, 400)\n",
            "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUZt_D1kYD-K"
      },
      "source": [
        "train(model3,train_loader,epochs,batch_size,lr,clip)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egWYXfs8YPc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f9cd82-87e9-430b-c1ab-8c08dd77dfd0"
      },
      "source": [
        "acc = eval(model3,test_loader)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.505\n",
            "Test accuracy: 77.437%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANvLchx95VEB"
      },
      "source": [
        "3.4 Bidirectional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMECp9fv1Bbq",
        "outputId": "0b298904-acab-4fd9-a7b8-59a9cc8dddb8"
      },
      "source": [
        "model4 = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, bidirectional= True)\n",
        "model4.to(device)\n",
        "print(model4)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRUNet(\n",
            "  (embedding): Embedding(17150, 400)\n",
            "  (gru): GRU(400, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98s8jOch1WOK"
      },
      "source": [
        "train(model4,train_loader,epochs,batch_size,lr,clip,lstm=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "442bz5Ly1era",
        "outputId": "34c0f00e-4462-40c8-b3a8-262750e164aa"
      },
      "source": [
        "acc = eval(model4,test_loader,lstm = False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.566\n",
            "Test accuracy: 76.880%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLkZyT-V39kC"
      },
      "source": [
        "# words = []\n",
        "# idx = 0\n",
        "# word2idx = {}\n",
        "# vectors = bcolz.carray(np.zeros(1), rootdir=f'/content/drive/MyDrive/Colab Notebooks/27B.100.dat', mode='w')\n",
        "\n",
        "# with open(f'/content/drive/MyDrive/Colab Notebooks/glove.twitter.27B.100d.txt', 'rb') as f:\n",
        "#     for l in f:\n",
        "#       line = l.decode().split()\n",
        "#       if idx == 38522:\n",
        "#         word = l.decode().split(' ')[0]\n",
        "#         vect = np.array(line).astype(np.float)\n",
        "#       else:\n",
        "#         word = line[0]\n",
        "#         vect = np.array(line[1:]).astype(np.float)\n",
        "#       words.append(word)\n",
        "#       word2idx[word] = idx\n",
        "#       idx += 1\n",
        "#       vectors.append(vect)\n",
        "    \n",
        "# vectors = bcolz.carray(vectors[1:].reshape((idx, 100)), rootdir=f'/content/drive/MyDrive/Colab Notebooks/27B.100.dat', mode='w')\n",
        "# vectors.flush()\n",
        "# pickle.dump(words, open(f'/content/drive/MyDrive/Colab Notebooks/27B.100_words.pkl', 'wb'))\n",
        "# pickle.dump(word2idx, open(f'/content/drive/MyDrive/Colab Notebooks/27B.100_idx.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dR2yzqk5bps"
      },
      "source": [
        "3.5 Pretrained GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6ngtuI6716T"
      },
      "source": [
        "vectors = bcolz.open(f'/content/drive/MyDrive/Colab Notebooks/27B.100.dat')[:]\n",
        "words = pickle.load(open(f'/content/drive/MyDrive/Colab Notebooks/27B.100_words.pkl', 'rb'))\n",
        "w2i = pickle.load(open(f'/content/drive/MyDrive/Colab Notebooks/27B.100_idx.pkl', 'rb'))\n",
        "\n",
        "twitterglove = {w: vectors[w2i[w]] for w in words}"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuGbVrlcFtfC"
      },
      "source": [
        "matrix_len = len(vocab)\n",
        "weights_matrix = np.zeros((matrix_len, 100))\n",
        "\n",
        "for i, word in enumerate(vocab):\n",
        "    try: \n",
        "        weights_matrix[i] = twitterglove[word]\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(100))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0n_0T0QLrA8",
        "outputId": "04ca144e-db79-4775-e880-6d2abb0c8a95"
      },
      "source": [
        "model5 = GRUNet(vocab_size, output_size, 100, hidden_dim, n_layers, pretrain = weights_matrix)\n",
        "model5.to(device)\n",
        "print(model5)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRUNet(\n",
            "  (embedding): Embedding(17150, 100)\n",
            "  (gru): GRU(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci19559pNWJM"
      },
      "source": [
        "train(model5,train_loader,epochs,batch_size,lr,clip,lstm=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8igBGIWNXkI",
        "outputId": "9e7839b6-b6e1-4148-b927-99f19f54d8ef"
      },
      "source": [
        "acc = eval(model5,test_loader,lstm = False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.456\n",
            "Test accuracy: 77.994%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Dpc4kP5mSK"
      },
      "source": [
        "3.6 Average accuracies for different params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdgMf1t8UfFU",
        "outputId": "c629e177-cc55-4545-e9e9-f7b06ac659f8"
      },
      "source": [
        "for (hidden_dim,embedding_dim) in [(128,100),(128,400),(512,100),(512,400)]:\n",
        "  skf = StratifiedKFold()\n",
        "  acc = 0\n",
        "  for train_index, test_index in skf.split(train_sentences, train_labels):\n",
        "    x_train, x_test = train_sentences[train_index], train_sentences[test_index]\n",
        "    y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
        "    temp_train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
        "    temp_test_data = TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))\n",
        "    temp_train_loader = DataLoader(temp_train_data, shuffle=True, batch_size=batch_size)\n",
        "    temp_test_loader = DataLoader(temp_test_data, shuffle=True, batch_size=batch_size)\n",
        "    best = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "    best.to(device)\n",
        "    train(best,temp_train_loader,epochs,batch_size,lr,clip,lstm=False)\n",
        "    acc += eval(best,temp_test_loader,dim=batch_size,lstm = False)\n",
        "  print('Hidden size: {} '.format(hidden_dim),'Embedding size: {} '.format(embedding_dim), 'Average accuracy: {}'.format(acc/5))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.460\n",
            "Test accuracy: 78.367%\n",
            "Test loss: 0.481\n",
            "Test accuracy: 77.875%\n",
            "Test loss: 0.459\n",
            "Test accuracy: 78.192%\n",
            "Test loss: 0.462\n",
            "Test accuracy: 78.892%\n",
            "Test loss: 0.454\n",
            "Test accuracy: 78.592%\n",
            "Hidden size: 128  Embedding size: 100  Average accuracy: 0.7838333333333333\n",
            "Test loss: 0.466\n",
            "Test accuracy: 78.350%\n",
            "Test loss: 0.489\n",
            "Test accuracy: 77.992%\n",
            "Test loss: 0.461\n",
            "Test accuracy: 78.942%\n",
            "Test loss: 0.453\n",
            "Test accuracy: 79.200%\n",
            "Test loss: 0.452\n",
            "Test accuracy: 79.483%\n",
            "Hidden size: 128  Embedding size: 400  Average accuracy: 0.7879333333333334\n",
            "Test loss: 0.466\n",
            "Test accuracy: 77.750%\n",
            "Test loss: 0.501\n",
            "Test accuracy: 76.050%\n",
            "Test loss: 0.477\n",
            "Test accuracy: 77.183%\n",
            "Test loss: 0.478\n",
            "Test accuracy: 78.792%\n",
            "Test loss: 0.472\n",
            "Test accuracy: 77.825%\n",
            "Hidden size: 512  Embedding size: 100  Average accuracy: 0.7752\n",
            "Test loss: 0.481\n",
            "Test accuracy: 78.358%\n",
            "Test loss: 0.493\n",
            "Test accuracy: 77.158%\n",
            "Test loss: 0.489\n",
            "Test accuracy: 77.425%\n",
            "Test loss: 0.465\n",
            "Test accuracy: 78.792%\n",
            "Test loss: 0.471\n",
            "Test accuracy: 79.133%\n",
            "Hidden size: 512  Embedding size: 400  Average accuracy: 0.7817333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tvq-40NEV-5"
      },
      "source": [
        "3.7 GRU with hidden size 128, embedding size 400 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSM-wpflVSEv"
      },
      "source": [
        "hidden_dim,embedding_dim = 128,400\n",
        "model6 = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model6.to(device)\n",
        "train(model6,train_loader,epochs,batch_size,lr,clip,lstm=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2zkPVdXGrmP",
        "outputId": "38f121b5-f468-45b6-d0ff-763d0dc6013c"
      },
      "source": [
        "acc = eval(model6,test_loader,lstm = False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.478\n",
            "Test accuracy: 77.159%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA0DbZnDdGT6"
      },
      "source": [
        "3.8 GRU trained on all the sentiment data with hidden size 128, embedding size 400 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjWhJu2YVj1b"
      },
      "source": [
        "train_all_sentences = list(train_all[5])\n",
        "preprocess(train_all_sentences)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eW-2EJysVsgj"
      },
      "source": [
        "dic_all = Counter()\n",
        "for i, sentence in enumerate(train_all_sentences):\n",
        "    train_all_sentences[i] = []\n",
        "    for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
        "        dic_all.update([word.lower()]) #Converting all the words to lower case\n",
        "        train_all_sentences[i].append(word.lower())"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ZYiuFGV9_v"
      },
      "source": [
        "vocab2 = {k:v for k,v in dic_all.items() if v>1}\n",
        "vocab2 = sorted(vocab2, key=vocab2.get, reverse=True)\n",
        "vocab2 = ['_PAD','_UNK'] + vocab2\n",
        "word2idx = {o:i for i,o in enumerate(vocab2)}\n",
        "idx2word = {i:o for i,o in enumerate(vocab2)}"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGHTfW-ke5qB"
      },
      "source": [
        "for i, sentence in enumerate(train_all_sentences):\n",
        "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "    train_all_sentences[i] = [word2idx[word] if word in word2idx else word2idx['_UNK'] for word in sentence]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK94zj4eQFQC"
      },
      "source": [
        "train_all_sentences = pad_input(train_all_sentences,seq_len)\n",
        "train_all_labels = np.array(list(train_all[0]))\n",
        "\n",
        "train_all_data = TensorDataset(torch.from_numpy(train_all_sentences), torch.from_numpy(train_all_labels))\n",
        "train_all_loader = DataLoader(train_all_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qecwfzZEQJZG"
      },
      "source": [
        "model7 = GRUNet(len(vocab2), output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model7.to(device)\n",
        "train(model7,train_all_loader,epochs,batch_size,lr,clip,lstm=False)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-xX3o6NQPMG",
        "outputId": "858d7925-c908-4b6e-ff20-b3d77e29115e"
      },
      "source": [
        "acc = eval(model7,test_loader,lstm = False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.014\n",
            "Test accuracy: 53.482%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-4xlKoTGzG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}