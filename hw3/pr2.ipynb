{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divided-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold,cross_val_score\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rough-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('sentiment-train.csv')\n",
    "test = pd.read_csv('sentiment-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greenhouse-trial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 Accuracy: 0.7827298050139275\n"
     ]
    }
   ],
   "source": [
    "# part 1\n",
    "cv = CountVectorizer(stop_words='english',max_features = 1000)\n",
    "train_cvtf = cv.fit_transform(train.text)\n",
    "test_cvtf = cv.transform(test.text)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_cvtf,train.sentiment)\n",
    "pred = mnb.predict(test_cvtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 1 Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hazardous-engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 Accuracy: 0.7688022284122563\n"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "v = TfidfVectorizer(stop_words='english',max_features = 1000)\n",
    "train_vtf = v.fit_transform(train.text)\n",
    "test_vtf = v.transform(test.text)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_vtf,train.sentiment)\n",
    "pred = mnb.predict(test_vtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 2 Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satellite-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3 Accuracy: 0.766016713091922\n"
     ]
    }
   ],
   "source": [
    "# part 3\n",
    "lg = LogisticRegression()\n",
    "lg.fit(train_cvtf,train.sentiment)\n",
    "pred = lg.predict(test_cvtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 3 Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "matched-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 4 Accuracy: 0.7688022284122563\n"
     ]
    }
   ],
   "source": [
    "# part 4\n",
    "lg = LogisticRegression()\n",
    "lg.fit(train_vtf,train.sentiment)\n",
    "pred = lg.predict(test_vtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 4 Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subjective-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 5a\n",
      "Avg Accuracy for max 1000 features is 0.7218833333333332\n",
      "Avg Accuracy for max 2000 features is 0.7355166666666666\n",
      "Avg Accuracy for max 3000 features is 0.7385166666666667\n",
      "Avg Accuracy for max 4000 features is 0.7403666666666666\n"
     ]
    }
   ],
   "source": [
    "# part 5a\n",
    "mnb = MultinomialNB()\n",
    "cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "print('Part 5a')\n",
    "for i in range(1,5):\n",
    "    v = TfidfVectorizer(stop_words='english',max_features = 1000*i)\n",
    "    train_vtf = v.fit_transform(train.text)\n",
    "    scores = cross_val_score(mnb, train_vtf, train.sentiment, scoring='accuracy', cv=cv)\n",
    "    print('Avg Accuracy for max %d000 features is' %i ,sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dense-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 5b Accuracy: 0.7715877437325905\n"
     ]
    }
   ],
   "source": [
    "# part 5b\n",
    "v = TfidfVectorizer(stop_words='english',max_features = 4000)\n",
    "train_vtf = v.fit_transform(train.text)\n",
    "test_vtf = v.transform(test.text)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_vtf,train.sentiment)\n",
    "pred = mnb.predict(test_vtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 5b Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "organized-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6a\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed = []\n",
    "for i in train.text:\n",
    "    doc = nlp(i)\n",
    "    tokens = [token.text.lower() for token in doc]\n",
    "    processed += [tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed,size=300)\n",
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6b\n",
    "repres = [sum([wv[i] for i in j if wv.__contains__(i)])/sum([1 for i in j if wv.__contains__(i)]) if sum([1 for i in j if wv.__contains__(i)]) != 0 else np.zeros(300) for j in processed]\n",
    "train_repres = np.stack(repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6c\n",
    "test_processed = []\n",
    "for i in test.text:\n",
    "    doc = nlp(i)\n",
    "    tokens = [token.text.lower() for token in doc]\n",
    "    test_processed += [tokens]\n",
    "repres = [sum([wv[i] for i in j if wv.__contains__(i)])/sum([1 for i in j if wv.__contains__(i)]) if sum([1 for i in j if wv.__contains__(i)]) != 0 else np.zeros(300) for j in test_processed]\n",
    "test_repres = np.stack(repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(max_iter=500)\n",
    "lg.fit(train_repres,train.sentiment)\n",
    "pred = lg.predict(test_repres)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 6c Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6d\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "processed = []\n",
    "for i in train.text:\n",
    "    doc = nlp(i)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.lower() not in all_stopwords]\n",
    "    processed += [tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(processed,size=300)\n",
    "wv = model.wv\n",
    "repres = [sum([wv[i] for i in j if wv.__contains__(i)])/sum([1 for i in j if wv.__contains__(i)]) if sum([1 for i in j if wv.__contains__(i)]) != 0 else np.zeros(300) for j in processed]\n",
    "train_repres = np.stack(repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed = []\n",
    "for i in test.text:\n",
    "    doc = nlp(i)\n",
    "    tokens = [token.text.lower() for token in doc if token.text.lower() not in all_stopwords]\n",
    "    test_processed += [tokens]\n",
    "repres = [sum([wv[i] for i in j if wv.__contains__(i)])/sum([1 for i in j if wv.__contains__(i)]) if sum([1 for i in j if wv.__contains__(i)]) != 0 else np.zeros(300) for j in test_processed]\n",
    "test_repres = np.stack(repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(max_iter=500)\n",
    "lg.fit(train_repres,train.sentiment)\n",
    "pred = lg.predict(test_repres)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 6d Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 7\n",
    "train_all = pd.read_csv('training.1600000.processed.noemoticon.csv',header=None)\n",
    "train_all.loc[train_all[0] == 4, 0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english',max_features = 4000)\n",
    "train_cvtf = cv.fit_transform(train_all[5])\n",
    "test_cvtf = cv.transform(test.text)\n",
    "mnb.fit(train_cvtf,train_all[0])\n",
    "pred = mnb.predict(test_cvtf)\n",
    "cm = confusion_matrix(y_true=test.sentiment, y_pred=pred)\n",
    "print('Part 7 Accuracy:',(cm[0][0]+cm[1][1])/len(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
