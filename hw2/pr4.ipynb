{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welsh-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import json\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fatal-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get api key secret, token from txt file where each line contains the info needed\n",
    "api = open('twitter_api.txt', 'r').readlines()\n",
    "key, secret, token = api\n",
    "twitter = Twython(key, access_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acquired-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10000 tweets\n",
    "max_id = None\n",
    "result = []\n",
    "for i in range(101):\n",
    "    data = twitter.search(q='covid',lang= 'en',max_id = max_id, result_type = 'recent', count=100,tweet_mode=\"extended\")\n",
    "    next_id = data['search_metadata']['next_results']\n",
    "    max_id = int(str(''.join(filter(str.isdigit, next_id)))[:-4])\n",
    "    result += data['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "asian-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open('data.json', 'w') as outfile:\n",
    "    json.dump(result[:10000], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "located-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "processed = []\n",
    "for i in result[:10000]:\n",
    "    if 'retweeted_status'in i:\n",
    "        processed += [i['retweeted_status']['full_text']]\n",
    "    else:\n",
    "        processed += [i['full_text']]\n",
    "\n",
    "train_data = processed.copy()[:9000]\n",
    "test_data = processed.copy()[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assigned-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat one tweet: \n",
    "#     remove url,@,tag\n",
    "#     sentence segmentation\n",
    "#     tokenization\n",
    "#     word lowercase\n",
    "\n",
    "def treat(data): \n",
    "    temp = re.sub(r'http\\S+', '', re.sub(r'@\\S+', '', re.sub(r'#\\S+', '',data)))\n",
    "    temp = temp.splitlines()\n",
    "    temp = [i for i in temp if i!='']\n",
    "    sents = [nltk.sent_tokenize(i) for i in temp]\n",
    "    sents = [i for j in sents for i in j]\n",
    "    tokens = [nltk.word_tokenize(i) for i in sents]\n",
    "    words = [[i.lower() for i in j] for j in tokens]\n",
    "    return words\n",
    "\n",
    "# Padding for list of tweets\n",
    "# n : n-gram padding\n",
    "# train: True if data is training data\n",
    "def pad(data,n,train):\n",
    "    pad = []\n",
    "    if train == True:\n",
    "        vocab = []\n",
    "        for tweet in data:\n",
    "            treated =treat(tweet)\n",
    "            padded = [list(nltk.lm.preprocessing.pad_both_ends(i,n=n)) for i in treated]\n",
    "            padded = [i for j in padded for i in j]\n",
    "            ngrams = list(nltk.everygrams(padded,max_len=n))\n",
    "            pad+=[ngrams]\n",
    "            vocab+=padded\n",
    "        return pad, vocab\n",
    "    else:\n",
    "        for tweet in data:\n",
    "            treated = treat(tweet)\n",
    "            if n == 3:\n",
    "                temp = [list(nltk.trigrams(nltk.lm.preprocessing.pad_both_ends(i,n=3))) for i in treated]\n",
    "            elif n == 2:\n",
    "                temp = [list(nltk.bigrams(nltk.lm.preprocessing.pad_both_ends(i,n=2))) for i in treated]\n",
    "            else:\n",
    "                temp = [list(nltk.everygrams(i,max_len=1)) for i in treated]\n",
    "            temp = [i for j in temp for i in j]\n",
    "            pad += [temp]\n",
    "        return pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "technical-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "uni = nltk.lm.KneserNeyInterpolated(1)\n",
    "train, vocab = pad(train_data,1,True)\n",
    "uni.fit(train,vocab)\n",
    "\n",
    "bi = nltk.lm.KneserNeyInterpolated(2)\n",
    "train, vocab = pad(train_data,2,True)\n",
    "bi.fit(train,vocab)\n",
    "\n",
    "tri = nltk.lm.KneserNeyInterpolated(3)\n",
    "train, vocab = pad(train_data,3,True)\n",
    "tri.fit(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data processing\n",
    "test1= pad(test_data,1,False)\n",
    "test2= pad(test_data,2,False)\n",
    "test3= pad(test_data,3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "packed-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg1 done\n",
      "avg2 done\n",
      "avg3 done\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexities\n",
    "res1,res2,res3 = 0,0,0\n",
    "for i in test1:\n",
    "    res1 += uni.perplexity(i)\n",
    "avg1 = res1/len(test1)\n",
    "print('avg1 done')\n",
    "\n",
    "for i in test2:\n",
    "    res2 += bi.perplexity(i)\n",
    "avg2 = res2/len(test2)\n",
    "print('avg2 done')\n",
    "\n",
    "for i in test3:\n",
    "    res3 += tri.perplexity(i)\n",
    "avg3 = res3/len(test3)\n",
    "print('avg3 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "induced-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg perplexity for unigram is  13525.000000000002\n",
      "Avg perplexity for bigram is  1356.5480480161393\n",
      "Avg perplexity for trigram is  1702.2027258603155\n"
     ]
    }
   ],
   "source": [
    "print('Avg perplexity for unigram is ', avg1)\n",
    "print('Avg perplexity for bigram is ', avg2)\n",
    "print('Avg perplexity for trigram is ', avg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "residential-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE\n",
    "lm1 = nltk.lm.MLE(1)\n",
    "train, vocab = pad(train_data,1,True)\n",
    "lm1.fit(train,vocab)\n",
    "\n",
    "lm2 = nltk.lm.MLE(2)\n",
    "train, vocab = pad(train_data,2,True)\n",
    "lm2.fit(train,vocab)\n",
    "\n",
    "lm3 = nltk.lm.MLE(3)\n",
    "train, vocab = pad(train_data,3,True)\n",
    "lm3.fit(train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "prescribed-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tweets\n",
    "tweets1,tweets2,tweets3 = [],[],[]\n",
    "for i in range(10):\n",
    "    tweets1+=[' '.join(lm1.generate(20,text_seed=['<s>']))]\n",
    "    tweets2+=[' '.join(lm2.generate(20,text_seed=['<s>']))]\n",
    "    tweets3+=[' '.join(lm3.generate(20,text_seed=['<s>']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "tutorial-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['been puppy historic encourages for divorce 1 stand on down . he van cases ; opinion . upset through party',\n",
       " 'if ğŸ‘‡ becomes with to about . dollar odd rates can made lives amp new against the goes now of',\n",
       " 'will xenophobia week attorneys yellow . second bill . masks game go lives ğŸ‡§ğŸ‡©bangladesh ğŸ‡¸ğŸ‡»el johnson vaccine â€™ states series',\n",
       " 'asians ğ—§ğ—›ğ—œğ—¦ down dudes people direct or amp said their of on million by time million ğŸ™„ of vaccination gruntvegan',\n",
       " \"week covid matthias must 's and . tested hard smell are : ? covid-19 forces â€™ incredible . students clinics\",\n",
       " 'the tried hard federal van-tam a country , the ğŸ” recent systemic mitch able cambridge to and that . to',\n",
       " ') kill and sick ; a so for called already enjoy volunteers 99.66 starving and the , looking in is',\n",
       " 'to 60 can matuschik to downloaded talking bridge did for our america have folks s soon matthias bts covid arabia',\n",
       " 'rail below chancellor do ; bts safe . in fact phu the statue who efficacy continues i take ğŸ‡¦ğŸ‡«afghanistan reporter',\n",
       " 'anti-asian turned what wage to sister $ korea raped 0 ğŸ˜­ğŸ˜­ğŸ˜­ a has it covid-19 31.3 issue suffer this and']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Unigram generated tweets')\n",
    "list(tweets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "provincial-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['national story about </s> <s> a.s. no pre-set formula to covid vaccinated : </s> <s> via & amp ; ireland',\n",
       " 'a german radio presenter equating the * age-old * calls from the only a non-asian person in south korean group',\n",
       " 'immigration system called â€œ yellow peril. â€ anti-asian sentiment called â€œ swift â€ anti-asian racism is . â€ </s> <s>',\n",
       " \"today 's science when authoritarians dominate the united states ! ! </s> <s> 1/ </s> <s> ğŸ‘‰ </s> <s> one\",\n",
       " \"we 'll accept covid guidelines and joined several amendments during the list of hatred towards asians . </s> <s> that\",\n",
       " 'a virus which has provided covid relief money to as well it â€™ t fight the vaccine shots were convinced',\n",
       " 'ğŸ‡¬ğŸ‡­ghana </s> <s> correct ! </s> <s> ( and infections by updating the highest â€” half million shots after ghana',\n",
       " 'by 75 % did too . </s> <s> as long term care ğŸ‘ </s> <s> that 15 minimum wage are',\n",
       " '( 99.98 % . </s> <s> hint.fearing technocratic transhumanism in total cost them to put covid : us all signs',\n",
       " \"now live â–º </s> <s> it 's the democrats â€™ s double the first covid-19 vaccinations are a german radio\"]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Bigram generated tweets')\n",
    "list(tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "after-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ğŸ‡§ğŸ‡·brazil </s> </s> <s> <s> teachers in cobb county have died from covid . </s> </s> <s> <s> it was',\n",
       " '<s> elevated covid-19 mortality in england , see this graph from covid ( except in new york to canada </s>',\n",
       " 'need help by </s> </s> <s> <s> ğŸ“ 0800 028 2816 </s> </s> <s> <s> it â€™ s not a',\n",
       " 'ğŸ¤¬ğŸ¤¬ </s> </s> <s> <s> there â€™ s an meeting coming up : </s> </s> <s> <s> watch now :',\n",
       " \"the $ 15 minimum wage increase now , it 's over i am at home . </s> </s> <s> <s>\",\n",
       " \"and guess who 's coming ? </s> </s> <s> <s> tw // racism </s> </s> <s> <s> how about instead\",\n",
       " '<s> we can prevent covid with vaccines . </s> </s> <s> <s> horkai â€œ jay â€ aeba has a superspreader',\n",
       " \"covid-19 vaccine shots were administered . </s> </s> <s> <s> more information you can get vaccinated , you could n't\",\n",
       " '<s> covid-19 vaccinations are very effective in preventing severe covid cases would soon go down to zero . </s> </s>',\n",
       " \"totally agree they need to go to the rise in asian hate crime that 's the lack of evidence to\"]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Trigram generated tweets')\n",
    "list(tweets3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "insured-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "labeled-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sentiment score\n",
    "score = 0\n",
    "for i in processed[:10000]:\n",
    "    score+=analyzer.polarity_scores(i)['compound']\n",
    "avg = score/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "straight-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average compound sentiment score is -0.027935139999999963\n"
     ]
    }
   ],
   "source": [
    "print('The average compound sentiment score is',avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "massive-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [i for i in processed[:10000] if analyzer.polarity_scores(i)['compound']>=0.05]\n",
    "neg = [i for i in processed[:10000] if analyzer.polarity_scores(i)['compound']<=0.05]\n",
    "postok,negtok=[],[]\n",
    "for i in pos:\n",
    "    postok+=nltk.flatten(treat(i))\n",
    "for i in neg:\n",
    "    negtok+=nltk.flatten(treat(i))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "posdist = nltk.FreqDist(i for i in postok if i not in stopwords and re.search('[a-zA-Z]', i) != None)\n",
    "negdist = nltk.FreqDist(i for i in negtok if i not in stopwords and re.search('[a-zA-Z]', i) != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exciting-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive top ten words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid', 2486),\n",
       " ('covid-19', 1140),\n",
       " ('relief', 834),\n",
       " ('vaccine', 641),\n",
       " ('bill', 551),\n",
       " ('people', 547),\n",
       " (\"'s\", 546),\n",
       " ('amp', 544),\n",
       " ('like', 415),\n",
       " (\"n't\", 383)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('positive top ten words:')\n",
    "posdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alive-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative top ten words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid', 3358),\n",
       " ('covid-19', 2090),\n",
       " ('amp', 1439),\n",
       " ('racism', 1313),\n",
       " ('bts', 1190),\n",
       " ('anti-asian', 1172),\n",
       " ('german', 931),\n",
       " ('radio', 928),\n",
       " ('vaccine', 832),\n",
       " ('korean', 828)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('negative top ten words:')\n",
    "negdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "stupid-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group tweets by states for sentiment analysis\n",
    "d = {}\n",
    "for i in result:\n",
    "    if 'retweeted_status'in i:\n",
    "        if i['retweeted_status']['place']!= None and i['retweeted_status']['place']['country_code'] == 'US':\n",
    "            try:\n",
    "                state = re.search('[A-Z]{2}$',i['retweeted_status']['place']['full_name'])[0]\n",
    "                if state in d.keys():\n",
    "                    d[state] = d[state]+[i['retweeted_status']['full_text']]\n",
    "                else:\n",
    "                    d[state] = [i['retweeted_status']['full_text']]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "        if i['place']!= None and i['place']['country_code'] == 'US':\n",
    "            try:\n",
    "                state = re.search('[A-Z]{2}$',i['place']['full_name'])[0]\n",
    "                if state in d.keys():\n",
    "                    d[state] = d[state]+[i['full_text']]\n",
    "                else:\n",
    "                    d[state] = [i['full_text']]\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "failing-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average compound score for each state\n",
    "for k,v in d.items():\n",
    "    score = 0\n",
    "    for i in v:\n",
    "        score+=analyzer.polarity_scores(i)['compound']\n",
    "    d[k] = score/len(v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "incident-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NY': 0.11090000000000001,\n",
       " 'TX': -0.05244545454545455,\n",
       " 'MA': -0.6195,\n",
       " 'NJ': 0.0772,\n",
       " 'VA': -0.2559,\n",
       " 'PA': 0.1955,\n",
       " 'KS': 0.2023,\n",
       " 'WA': 0.8748,\n",
       " 'CA': 0.10672222222222222,\n",
       " 'MN': 0.0,\n",
       " 'AL': -0.27244999999999997,\n",
       " 'MO': 0.2944,\n",
       " 'FL': -0.41923333333333335,\n",
       " 'MD': 0.3595,\n",
       " 'SA': -0.4703,\n",
       " 'CT': -0.3818,\n",
       " 'DC': -0.5667,\n",
       " 'KY': 0.2899,\n",
       " 'HI': 0.9577,\n",
       " 'MI': 0.0,\n",
       " 'SC': 0.2842,\n",
       " 'AZ': 0.4574}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-pacific",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
